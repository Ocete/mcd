{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qncY3FktdgMI"
   },
   "source": [
    "# PIT - Práctica 3: Detección de Actividad de Voz (VAD)\n",
    "\n",
    "**Alicia Lozano Díez** y **Pablo Ramírez Hereza**\n",
    " \n",
    "7 de marzo de 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6yNO58J-7my"
   },
   "source": [
    "## Objetivo\n",
    "\n",
    "El objetivo de esta práctica es proporcionar una introducción al procesamiento de señales temporales de voz, y desarrollar de un detector de actividad de voz basado en redes neuronales recurrentes, en particular, LSTM.\n",
    "\n",
    "### Materiales\n",
    "\n",
    "- Guión (.ipynb) de la práctica - Moodle\n",
    "- Ejemplos de datos y etiquetas - Moodle\n",
    "- Listas de entrenamiento y validación - Moodle\n",
    "- Scripts de descarga de datos - Moodle\n",
    "- Datos y etiquetas de entrenamiento * - One Drive (https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/EdCueYU7BpNAuo6BawH8hJAB5rclap745BmsPzXgSPhsgw?e=Fh9adH)\n",
    "- Datos y etiquetas de validación * - One Drive (https://dauam-my.sharepoint.com/:u:/g/personal/alicia_lozano_uam_es/EWBjWyX774pLhJc2ahr4zk0BtLvWt7YGhdMDDmGu-LcBNQ?e=sbgtjF)\n",
    "\n",
    "\n",
    "**CUIDADO: * Los datos proporcionados son de uso exclusivo para esta práctica. No tiene permiso para copiar, distribuir o utilizar el corpus para ningún otro propósito.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1BzhOOn-6H4"
   },
   "source": [
    "# 1. Introducción al procesamiento de señales temporales de voz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tgJsLk6UQQG"
   },
   "source": [
    "## 1.1. Descarga de ficheros de ejemplo\n",
    "\n",
    "Primero vamos a descargar el audio de ejemplo de Moodle (**audio_sample.wav**) y ejecutar las siguientes  líneas de código, que nos permitirán subir el archivo a Google Colab desde el disco local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edF0oDBFruUG"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiWqca8brveq"
   },
   "source": [
    "Una vez cargado el fichero de audio, podemos escucharlo de la siguiente manera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1UH-jL-DBPpU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_sample.wav\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "rate must be specified when data is a numpy array or list of audio samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7028/698368746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwav_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"audio_sample.wav\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ocete\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\lib\\display.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "wav_file_name = \"audio_sample.wav\"\n",
    "print(wav_file_name)\n",
    "IPython.display.Audio(wav_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAujomkYr41Q"
   },
   "source": [
    "## 1.2. Lectura y representación de audio en Python\n",
    "\n",
    "A continuación vamos a definir ciertas funciones para poder hacer manejo de  ficheros de audio en Python. \n",
    "\n",
    "Comenzamos definiendo una función **read_recording** que leerá un fichero de audio WAV, normalizará la amplitud y devolverá el vector de muestras _signal_ y su frecuencia de muestreo _fs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvbxNBIGBi1O"
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "\n",
    "def read_recording(wav_file_name): \n",
    "  fs, signal = scipy.io.wavfile.read(wav_file_name)\n",
    "  signal = signal/max(abs(signal)) # normalizes amplitude\n",
    "  \n",
    "  return fs, signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QESNJAasdB5"
   },
   "source": [
    "Si ejecutamos la función anterior para el fichero de ejemplo, podemos ver la forma en la que se carga dicho fichero de audio en Python. Así, podemos obtener la frecuencia de muestreo y la longitud del fichero en número de muestras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISH_GeSReo8i"
   },
   "outputs": [],
   "source": [
    "fs, signal = read_recording(wav_file_name) \n",
    "print(\"Signal variable shape: \" + str(signal.shape))\n",
    "print(\"Sample rate: \" + str(fs))\n",
    "print(\"File length: \" + str(len(signal)) + \" samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njGccLOJvoWe"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "\n",
    "- ¿Como obtendría la duración de la señal en segundos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt1f-HXntuLS"
   },
   "source": [
    "También podemos representar la señal y ver su forma de onda. Para ello, definimos la función **plot_signal** como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOzyL0JXCG65"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_signal(signal, fs, ylabel=\"\", title=\"\"):\n",
    "  dur = len(signal)/fs\n",
    "  step = 1./fs\n",
    "  t_axis = np.arange(0., dur, step)\n",
    "\n",
    "  plt.plot(t_axis, signal)\n",
    "  plt.xlim([0, dur])\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.xlabel('Time (seconds)')\n",
    "  plt.title(title)\n",
    "  plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAW3wzOxuAB-"
   },
   "source": [
    "Y utilizando la función anterior, obtenemos su representación (amplitud frente al tiempo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpjB716Yma3O"
   },
   "outputs": [],
   "source": [
    "plot_signal(signal, fs, \"Amplitude\", wav_file_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZp7lFEEv-yO"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- Incluya en el informe la representación obtenida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAzKKzBiuTWO"
   },
   "source": [
    "## 1.3. Representación de etiquetas de actividad de voz\n",
    "\n",
    "En esta práctica, vamos a desarrollar un detector de actividad de voz, que determinará qué segmentos de la señal de voz son realmente voz y cuáles silencio.\n",
    "\n",
    "Por ello, vamos a ver dos ejemplos de etiquetas _ground truth_, que corresponden al fichero de audio de ejemplo. \n",
    "\n",
    "Primero, descargamos de Moodle las etiquetas de voz/silencio que están en los ficheros **audio_sample_labels_1.voz** y **audio_sample_labels_2.voz** y las cargamos en Google Colab como en el caso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM5RhBFwCx3-"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1gohRGQxPFq"
   },
   "source": [
    "Estas etiquetas están guardadas en ficheros de texto y podemos cargarlas en Python de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4g5HQm6KNQL"
   },
   "outputs": [],
   "source": [
    "labels_file_name = 'audio_sample_labels_1.voz'\n",
    "voice_labels = np.loadtxt(labels_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOarG7qcx7BV"
   },
   "source": [
    "Con el siguiente código, podemos representar la señal de voz así como sus etiquetas en la misma figura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "telfETsXx5GK"
   },
   "outputs": [],
   "source": [
    "plot_signal(signal, fs)\n",
    "plot_signal(voice_labels*2-1, fs, \"Amplitude\", wav_file_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eW_pmANycy7"
   },
   "source": [
    "Las etiquetas de voz/silencio provienen de distintos detectores de actividad de voz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl3hGkNIxmuT"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- ¿Qué valores tienen las etiquetas? ¿Qué significan dichos valores?\n",
    "- ¿Por qué se representa _voice_labels*2-1_?\n",
    "- Represente la señal de voz junto con las etiquetas para ambos casos e incluya las figuras en el informe de la práctica. ¿Qué diferencias observas? ¿A qué se puede deber?\n",
    "- ¿Qué cantidad de voz/silencio hay en cada etiquetado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHj1_1fHzAed"
   },
   "source": [
    "## 1.4. Extracción de características\n",
    "\n",
    "En la mayoría de sistemas de reconocimiento de patrones, un primer paso es la extracción de características. Esto consiste, a grandes rasgos, en obtener una representación de los datos de entrada, que serán utilizados para un posterior modelado. \n",
    "\n",
    "En nuestro caso, vamos pasar de la señal en crudo _\"raw\"_ dada por las muestras (_signal_), a una secuencia de vectores de características que extraigan información a corto plazo de la misma y la representen. Esta sería la entrada a nuestro sistema de detección de voz basado en redes neuronales.\n",
    "\n",
    "Para ver algunos ejemplos, vamos a utilizar la librería _librosa_ (https://librosa.org/doc/latest/index.html). \n",
    "\n",
    "Dentro de esta librería, tenemos funciones para extraer distintos tipos de características de la señal de voz, como por ejemplo el espectrograma en escala Mel (_melspectrogram_). \n",
    "\n",
    "Estas características a corto plazo, se extraen en ventanas de unos pocos milisegundos con o sin solapamiento. \n",
    "\n",
    "Un ejemplo sería el siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a_7VvYYMFnQ"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "mel_spec = librosa.feature.melspectrogram(signal,fs,n_mels=23,win_length=320,hop_length=160)\n",
    "\n",
    "print(mel_spec.shape)\n",
    "print(signal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQPnf6CB1Dqh"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- ¿Qué se obtiene de la función anterior? \n",
    "- ¿Qué significan los valores de los parámetros _win_length_ y _hop_length_?\n",
    "- ¿Qué dimensiones de _mel_spec_ obtienes? ¿Qué significan? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MwMMOuI1icD"
   },
   "source": [
    "De esta manera, podríamos obtener una parametrización de las señales para ser utilizadas como entrada a nuestra red neuronal. \n",
    "\n",
    "Para los siguientes apartados, se proporcionan los vectores de características MFCC para una serie de audios que se utilizarán como conjunto de entrenamiento del modelo de VAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64IMtv3LPTIX"
   },
   "source": [
    "#2. Detector de actividad de voz (Voice Activity Detector, VAD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWgXfSysUTqQ"
   },
   "source": [
    "\n",
    "## 2.1. Descarga de los datos de entrenamiento\n",
    "\n",
    "Primero vamos a descargar la lista de identificadores de los datos de entrenamiento de la práctica. \n",
    "\n",
    "Para ello, necesitaremos descargar de Moodle el fichero **training_VAD.lst**, y ejecutar las siguientes líneas de código, que nos permitirán cargar el archivo a Google Colab desde el disco local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SB8wcxZxVS9g"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTthDBqzPxen"
   },
   "source": [
    "A continuación cargamos los identificadores contenidos en el fichero en una lista en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOHSTbDiVB1y"
   },
   "outputs": [],
   "source": [
    "file_train_list = 'training_VAD.lst' # mat files containing data + labels\n",
    "f = open(file_train_list, 'r')\n",
    "train_list = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfBy_23fP-Cr"
   },
   "source": [
    "Podemos ver algunos de ellos (los primeros 10 identificatores) de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLIzBjxNQB9I"
   },
   "outputs": [],
   "source": [
    "print(train_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8Bexbe3QSpW"
   },
   "source": [
    "Ahora, descargaremos de Moodle el fichero **data_download_onedrive_training_VAD.sh**, y ejecutaremos las siguientes líneas de código, que nos permitirán cargar el archivo a Google Colab desde el disco local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHJX10CN3KhK"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gonGnfmiQhmR"
   },
   "source": [
    "Para descargar el conjunto de datos desde One drive, ejecutamos el script cargado anteriormente de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2NmZBo-34xk"
   },
   "outputs": [],
   "source": [
    "!chmod 755 data_download_onedrive_training_VAD.sh\n",
    "!./data_download_onedrive_training_VAD.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkshERjWQx_-"
   },
   "source": [
    "Este script descargará los datos de One Drive y los cargará en Google Colab, descomprimiéndolos en la carpeta **data/training_VAD**.\n",
    "\n",
    "Podemos comprobar que los ficheros **.mat** se encuentran en el directorio esperado:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6Cr06cf4pe0"
   },
   "outputs": [],
   "source": [
    "!ls data/training_VAD/ | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTwj-3NjSpJx"
   },
   "source": [
    "## 2.2. Definición del modelo\n",
    "\n",
    "Utilizando la librería Pytorch (https://pytorch.org/docs/stable/index.html), vamos a definir un modelo de ejemplo con una capa LSTM y una capa de salida. La capa de salida estará formada por una única neurona. La salida indicará la probabilidad de voz/silencio utilizando una función *sigmoid*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUBhgYruUKfI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self, feat_dim=20):\n",
    "        super(Model_1, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(feat_dim,256,batch_first=True,bidirectional=False)\n",
    "        self.output = nn.Linear(256,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.lstm(x)[0]\n",
    "        out = self.output(out)\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        return out.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-pK445hTOvq"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- ¿Qué tamaño tiene la entrada a la capa LSTM? \n",
    "- ¿Cuántas unidades (celdas) tiene dicha capa LSTM?\n",
    "- ¿Qué tipo de matriz espera la LSTM? Mirar la documentación y describir brevemente.\n",
    "- Revisar la documentación de _torch.nn.LSTM_ y describir brevemente los argumentos _batch_first_, _bidirectional_ y _dropout_.\n",
    "- En este modelo, estamos utilizando una única neurona a la salida. ¿Hay alguna otra alternativa? ¿Se seguiría utilizando una función _sigmoid_?\n",
    "- ¿Para qué sirve la función _forward_ definida en la clase _Model_1_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1M06GDqU31I"
   },
   "source": [
    "Una vez definida la clase, podemos crear nuestra instancia del modelo y cargarlo en la GPU con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGPBGVSkUacV"
   },
   "outputs": [],
   "source": [
    "model = Model_1(feat_dim=20)\n",
    "model = model.to(torch.device(\"cuda\"))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC2BAp7tVC1A"
   },
   "source": [
    "Nuestra variable _model_ contiene el modelo, y ya estamos listos para entrenarlo y evaluarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZFacm445T6X"
   },
   "source": [
    "##2.3. Lectura y preparación de los datos para el entrenamiento\n",
    "\n",
    "Como hemos visto anteriormente, nuestros datos están guardados en ficheros de Matlab (**.mat**). Cada uno de estos ficheros contiene una matriz **X** correspondiente a las secuencias de características MFCC (con sus derivadas de primer y segundo orden), y un vector **Y** con las etiquetas de voz/silencio correspondientes.\n",
    "\n",
    "Veamos un ejemplo: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwLhQiBbVnpZ"
   },
   "outputs": [],
   "source": [
    "features_file = 'data/training_VAD/features_labs_1.mat'\n",
    "\n",
    "import scipy.io\n",
    "features = scipy.io.loadmat(features_file)['X']\n",
    "labels = scipy.io.loadmat(features_file)['Y']\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BYGGYYiWNgV"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "Elegir un fichero de entrenamiento y responder a las siguientes preguntas: \n",
    "- ¿Qué tamaño tiene **features**? ¿Y **labels**?\n",
    "- Una de las dimensiones de la **features** es 60, correspondiente a los 20 coeficientes MFCC concatenados con las derivadas de primer y segundo orden. ¿Con qué se corresponde la otra dimensión?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEhUjoWrXpBj"
   },
   "source": [
    "El entrenamiento del modelo se va a realizar mediante descenso por gradiente (o alguna de sus variantes) basado en _batches_. \n",
    "\n",
    "Para preparar cada uno de estos _batches_ que servirán de entrada a nuestro modelo LSTM, debemos almacenar las características en secuencias de la misma longitud. El siguiente código lee las características (**get_fea**) y sus correspondientes etiquetas (**get_lab**) de un fragmento aleatorio del fichero de entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNAp_LJiUjAV"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "def get_fea(segment, rand_idx):\n",
    "    data = scipy.io.loadmat(segment)['X']\n",
    "    if data.shape[0] <= length_segments:\n",
    "        start_frame = 0\n",
    "    else:\n",
    "        start_frame = np.random.permutation(data.shape[0]-length_segments)[0]\n",
    "\n",
    "    end_frame = np.min((start_frame + length_segments,data.shape[0]))\n",
    "    rand_idx[segment] = start_frame\n",
    "    feat = data[start_frame:end_frame,:20] # discard D and DD, just 20 MFCCs\n",
    "    return feat[np.newaxis, :, :]   \n",
    "\n",
    "\n",
    "def get_lab(segment, rand_idx):\n",
    "    data = scipy.io.loadmat(segment)['Y']\n",
    "    start_frame = rand_idx[segment]\n",
    "    end_frame = np.min((start_frame + length_segments, data.shape[0]))\n",
    "    labs = data[start_frame:end_frame].flatten()\n",
    "    return labs[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wek4EaRoanBq"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "Analizar las funciones anteriores detenidamente y responder a las siguientes cuestiones:\n",
    "- ¿De qué tamaño son los fragmentos que se están leyendo?\n",
    "- ¿Para qué sirve _rand_idx_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrBM4q80bBEK"
   },
   "source": [
    "## 2.4. Entrenamiento del modelo\n",
    "Una vez definidas las funciones de lectura de datos y preparación del formato que necesitamos para la entrada a la red LSTM, podemos utilizar el siguiente código para entrenarlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xDupSk1U2li"
   },
   "outputs": [],
   "source": [
    "length_segments = 300 \n",
    "path_in_feat = 'data/training_VAD/'\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 51\n",
    "segment_sets = np.array_split(train_list, len(train_list)/batch_size)\n",
    "\n",
    "max_iters = 5\n",
    "for epoch in range(1, max_iters):\n",
    "  print('Epoch: ',epoch)\n",
    "  model.train()\n",
    "  cache_loss = 0\n",
    "\n",
    "  for ii, segment_set in enumerate(segment_sets):\n",
    "    rand_idx = {}\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Create training batches\n",
    "    train_batch = np.vstack([get_fea(path_in_feat + segment, rand_idx) for segment in segment_set])\n",
    "    labs_batch = np.vstack([get_lab(path_in_feat + segment, rand_idx).astype(np.int16)  for segment in segment_set])\n",
    "    assert len(labs_batch) == len(train_batch) # make sure that all frames have defined label\n",
    "    # Shuffle the data and place them into Pytorch tensors\n",
    "    shuffle = np.random.permutation(len(labs_batch))\n",
    "    labs_batch = torch.tensor(labs_batch.take(shuffle, axis=0).astype(\"float32\")).to(torch.device(\"cuda\"))\n",
    "    train_batch = torch.tensor(train_batch.take(shuffle, axis=0).astype(\"float32\")).to(torch.device(\"cuda\"))\n",
    "\n",
    "    # Forward the data through the network\n",
    "    outputs = model(train_batch)\n",
    "\n",
    "    # Compute cost\n",
    "    loss = criterion(outputs, labs_batch)\n",
    "\n",
    "    # Backward step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cache_loss += loss.item()\n",
    "\n",
    "  print(\"Loss: \" + str(cache_loss/len(train_batch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h52xtcubSnD"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "Analizar el código anterior cuidadosamente y ejecutarlo. A continuación, responder a las siguientes cuestiones:\n",
    "- ¿Qué función de coste se está optimizando? Describir brevemente con ayuda de la documentación. \n",
    "- ¿Qué optimizador se ha definido?\n",
    "- ¿Para qué se utiliza _batch_size_?\n",
    "- Describir brevemente la creación de los _batches_.\n",
    "- ¿Qué línea de código realiza el _forward pass_?\n",
    "- ¿Qué línea de código realiza el _backward pass_?\n",
    "- ¿Cuántas iteraciones del algoritmo ha realizado? ¿Qué observa en la evolución de la función de coste?\n",
    "- Añada al código el cálculo de la precisión o _accuracy_, de tal manera que se muestre por pantalla dicho valor en cada iteración (similar a lo que ocurre con el valor del coste _loss_). Copiar el código en el informe y describir brevemente.\n",
    "- ¿Qué valor de coste y _accuracy_ obtiene? ¿Cómo se puede mejorar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_LnYNdQeA91"
   },
   "source": [
    "## 2.5. Evaluación del modelo: un único fichero de test\n",
    "\n",
    "Una vez entrenado el modelo, vamos a evaluarlo en un ejemplo en concreto.\n",
    "\n",
    "Descargue de Moodle el fichero **audio_sample_test.wav**, con sus correspondientes características y etiquetas **audio_sample_test.mat** y evalúe el rendimiento en el mismo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwSqBU1zej_R"
   },
   "source": [
    "\n",
    "**PREGUNTAS:**\n",
    "- Incluya en el informe de la práctica el código que ha utilizado para evaluar dicho fichero.\n",
    "- ¿Cuál es el _accuracy_ obtenido para el fichero **audio_sample_test**?\n",
    "- Represente 10 segundos de dicho audio, así como sus etiquetas de _ground_truth_ y las obtenidas con su modelo. Incluya dicha gráfica en el informe y comente brevemente el resultado. Visualmente, ¿es bueno el modelo? \n",
    "- Escuche el audio y comente cualitativamente cómo es de bueno o malo el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "722dWQLvQDB7"
   },
   "source": [
    "## 2.6. Evaluación del modelo: conjunto de validación\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBKhlbtOR1Rt"
   },
   "source": [
    "Ahora vamos a evaluar el rendimiento del modelo anterior sobre un conjunto de validación (del que conocemos sus etiquetas).\n",
    "\n",
    "Para este conjunto de datos, descargaremos la lista de identificadores **valid_VAD.lst** de Moodle, así como el fichero de descarga de datos **data_download_onedrive_valid_VAD.sh**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJs0XS6rSIOf"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "!chmod 755 data_download_onedrive_valid_VAD.sh\n",
    "!./data_download_onedrive_valid_VAD.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lDpCsrORqqj"
   },
   "source": [
    "Escriba ahora el código necesario para evaluar el modelo anterior en el conjunto de datos de validación, para su última época. \n",
    "\n",
    "Tenga en cuenta que si quiere realizar el forward para todos los datos de validación de una vez, necesitará que todas las secuencias sean de la misma longitud. Como aproximación, puede escoger unos pocos segundos de cada fichero como se hace en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmiIbHbKRvON"
   },
   "outputs": [],
   "source": [
    "# INSERTE SU CÓDIGO AQUÍ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJ4V2xRvRmcE"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- Incluya en la memoria de la práctica el código utilizado, incluyendo los valores de cualquier parámetro de configuración utilizado (por ejemplo, el número de épocas de entrenamiento realizadas).\n",
    "- ¿Qué rendimiento (loss y accuracy) obtiene con este modelo (_Model_1_) en entrenamiento y en validación? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsVxachwSa0z"
   },
   "source": [
    "# 3. Comparación de modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEUWfJmKULm6"
   },
   "source": [
    "\n",
    "## 3.1. Redes LSTM bidireccionales\n",
    "\n",
    "En este apartado, vamos a partir del modelo inicial (_Model_1_) y modificarlo para que la capa LSTM sea bidireccional (_Model_1B_). \n",
    "\n",
    "Entrene el nuevo modelo y compare el resultado con el modelo inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4gAYAiUTGsB"
   },
   "outputs": [],
   "source": [
    "# INSERTE SU CÓDIGO AQUÍ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1Iv9U1ZTFzP"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- Explique brevemente la diferencia entre una capa LSTM y una BLSTM (bidirectional LSTM).\n",
    "- Incluya el código donde define _Model_1B_ en el informe de la práctica.\n",
    "- ¿Qué modelo obtiene un mejor resultado sobre los datos de validación? ¿Por qué puede ocurrir esto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCYUXhQHTTH6"
   },
   "source": [
    "## 3.2. Modelo \"más profundo\"\n",
    "\n",
    "En este apartado, vamos a partir nuevamente del modelo _Model_1_ y vamos a añadir una segunda capa LSTM tras la primera, con el mismo tamaño y configuración, definiendo un nuevo modelo _Model_2_.\n",
    "\n",
    "Entrénelo y compare los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG-87vDhTeHB"
   },
   "outputs": [],
   "source": [
    "# INSERTE SU CÓDIGO AQUÍ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEnYNMZ1TjOa"
   },
   "source": [
    "**PREGUNTAS:**\n",
    "- Incluya el código de la clase _Model_2_ en la memoria.\n",
    "- ¿Qué modelo obtiene un mejor resultado sobre los datos de validación, _Model_1_ o _Model_2_? ¿Por qué puede ocurrir esto?\n",
    "- Y con respecto a _Model_1B_, ¿cuál es mejor?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "PR3_PIT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
